import data.tags_processed
import modules.quantize

train.iterations=400000
train.learning_rate=0.00028
train.weight_decay=0.015
train.batch_size=128
train.vae_input_dim=768
train.vae_n_cat_feats=0
train.vae_hidden_dims=[512, 256]
train.vae_embed_dim=64
train.vae_codebook_size=256
train.vae_codebook_normalize=True
train.vae_sim_vq=False
train.save_model_every=5000
train.eval_every=10000
train.dataset_folder="/root/MedRQ/dataset/mimic-iii_data/ontology/diag"  

train.dataset=%data.tags_processed.RecDataset.MEDICAL_MIMIC3
train.save_dir_root="out_mimic3_icd/medrq/medical/"
train.commitment_weight=0.25
train.vae_n_layers=3
train.vae_codebook_mode=%modules.quantize.QuantizeForwardMode.ROTATION_TRICK

train.use_kmeans_init=True

train.layer_specific_lr=True
train.predictor_weight_decay=0.015
train.gradient_accumulate_every=2

train.tag_alignment_weight=1
train.tag_prediction_weight=1
# 与已生成的标签词表规模一致
train.tag_class_counts=[3, 43, 206]
train.tag_embed_dim = 768

train.force_dataset_process=False
train.do_eval=True

train.sem_id_uniqueness_weight=0
train.sem_id_uniqueness_margin=0.8
train.id_repetition_threshold=0.06

train.rare_tag_threshold=3
train.use_focal_loss=True
train.focal_loss_gamma_base=2.5
train.focal_loss_alpha_base=0.24

train.dropout_rate=0.4
train.use_batch_norm=True
train.alignment_temperature=0.1

train.use_label_smoothing=True
train.label_smoothing_alpha=0.13
train.use_mixup=True
train.mixup_alpha=0.2

train.eval_tta=True
train.eval_temperature=0.07
train.ensemble_predictions=True

train.use_lr_scheduler=True
train.lr_scheduler_type='cosine'
train.lr_scheduler_T_max=400000
train.lr_scheduler_eta_min=7e-8
train.lr_scheduler_step_size=100000
train.lr_scheduler_gamma=0.5
train.lr_scheduler_factor=0.5
train.lr_scheduler_patience=10
